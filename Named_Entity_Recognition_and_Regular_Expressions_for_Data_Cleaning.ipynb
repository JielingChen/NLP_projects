{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adYQ18t9lEuB"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "This project used a combination of spaCy's Named Entity Recognition (NER) model and regular expressions to automate the extraction of client names and proposal dates from thousands of service proposals in an accounting firm's proposal archive. This significantly reduced the amount of manual effort required for data cleaning. The extracted information was then saved to an Excel file for future use."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pdfminer.six"
      ],
      "metadata": {
        "id": "lomts0lSntd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy"
      ],
      "metadata": {
        "id": "0xAZeooGoNnB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_lg"
      ],
      "metadata": {
        "id": "4GxmCahWn27m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bIFes32QlEuG"
      },
      "outputs": [],
      "source": [
        "# import spaCy for NLP\n",
        "import spacy\n",
        "\n",
        "# import os to get the local pdf files\n",
        "import os\n",
        "\n",
        "# import StringIO and pdfminer to extract text from pdf \n",
        "from io import StringIO\n",
        "from pdfminer.high_level import extract_text_to_fp\n",
        "\n",
        "# import regular expression to find pattern in text\n",
        "import re\n",
        "\n",
        "# import pandas to export the data into an excel file\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fDG3zPGGlEuI"
      },
      "outputs": [],
      "source": [
        "# a function to extract text from user-defined pages of a pdf file\n",
        "def extract_text_from_pages(file, *args):\n",
        "    try:\n",
        "        # open pdf file\n",
        "        with open(file, 'rb') as file:\n",
        "          \n",
        "            # extract text from pdf\n",
        "            string_io = StringIO()\n",
        "            extract_text_to_fp(file, string_io, page_numbers = [arg-1 for arg in args])\n",
        "            extracted_text = string_io.getvalue()\n",
        "\n",
        "            # clean text - replace multiple whitespaces characters with a single whitespace\n",
        "            extracted_text = re.sub(r'\\s+', ' ', extracted_text)\n",
        "            \n",
        "    except:\n",
        "        # the message when pdfminer fails to extract text from pdf\n",
        "        extracted_text = 'PDF Access Denied'\n",
        "    \n",
        "    return extracted_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aH-lDS97lEuI"
      },
      "outputs": [],
      "source": [
        "# load spacy pre-trained model\n",
        "nlp = spacy.load('en_core_web_lg')\n",
        "\n",
        "# get the \"Named Entity Recognition\" pipeline\n",
        "ner = nlp.get_pipe('ner')\n",
        "\n",
        "# configure the pipeline to lower threshold to make it more sensitive to a potential organization name\n",
        "ner.cfg['threshold'] = 0.01"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uKA9w-oylEuJ"
      },
      "outputs": [],
      "source": [
        "# directory path where the PDF files are stored\n",
        "pdf_dir = 'C:/Users/directory/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSZuK1GwlEuJ"
      },
      "source": [
        "## 1. Extract Client Names From Proposals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U40Aonn3lEuK"
      },
      "outputs": [],
      "source": [
        "# a function to clean the organization names for better named entity recognition\n",
        "def clean_organization_name(text):\n",
        "    \n",
        "    # clean the text\n",
        "    text = text.replace(',LLC', ' LLC')\n",
        "    text = text.replace(', LLC', ' LLC')\n",
        "    text = text.replace(',LP', ' L.P.')\n",
        "    text = text.replace(', LP', ' L.P.')\n",
        "    text = text.replace(' LP', ' L.P.')\n",
        "    text = text.replace(', L.P.', ' L.P.')\n",
        "    text = text.replace(',L.P.', ' L.P.')\n",
        "    text = text.replace(', Inc', ' Inc')\n",
        "    cleaned_text = text.replace(',Inc', ' Inc')\n",
        "    \n",
        "    return cleaned_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YIclU82xlEuJ"
      },
      "outputs": [],
      "source": [
        "# a function to run NER model on a text to extract entities labeled 'ORG'\n",
        "def ner_org(text):\n",
        "    \n",
        "    # use spaCy nlp pipeline to analyze the text                \n",
        "    doc = nlp(text)\n",
        "                    \n",
        "    # extract the named entities labeled 'ORG' and exclude the service provider's name\n",
        "    organizations = set([ent.text for ent in doc.ents if ent.label_ == 'ORG' and 'provider_name' not in ent.text.lower()])\n",
        "\n",
        "    return organizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQyxtOgnlEuK"
      },
      "outputs": [],
      "source": [
        "# a combined function\n",
        "def clean_and_ner_org(text):\n",
        "    \n",
        "    cleaned_text = clean_organization_name(text)\n",
        "    organizations = ner_org(cleaned_text)\n",
        "    \n",
        "    return organizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_hVIy-b4lEuL"
      },
      "outputs": [],
      "source": [
        "# define the patterns we want to identify in the text\n",
        "\n",
        "# 1: \"Dear xxx: We are delighted to have this opportunity to propose on audit and tax services for XYZ(client name)\"\n",
        "pattern_1 = r'Dear(.*?)\\('\n",
        "\n",
        "# 2: In the cover page(first page) \"XYZ(client name) Proposal to Provide Professional Services\"\n",
        "pattern_2 = r'.+(?=Proposal)'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FOl76pH3lEuL"
      },
      "outputs": [],
      "source": [
        "# create an empty list to store the data that need to be exported to excel\n",
        "data_organization = []\n",
        "\n",
        "# loop through all pdf files in a folder\n",
        "for filename in os.listdir(pdf_dir):\n",
        "    if filename.endswith('.pdf'):\n",
        "        \n",
        "        # extract the first 2 pages\n",
        "        extracted_text = extract_text_from_pages(pdf_dir + filename, 1, 2)\n",
        "        \n",
        "        # check if we can extract text from the pdf file\n",
        "        if extracted_text == 'PDF Access Denied':\n",
        "            # put the message into the data list\n",
        "            data_organization.append([filename, extracted_text, extracted_text])\n",
        "        \n",
        "        else:\n",
        "            # search for the pattern_1 in the first 2 pages\n",
        "            match_1 = re.search(pattern_1, extracted_text)\n",
        "                \n",
        "            if match_1:\n",
        "                # get the matched text\n",
        "                match_1_text = match_1.group(0)\n",
        "                \n",
        "                # clean the text and identify organizations\n",
        "                organizations = clean_and_ner_org(match_1_text)\n",
        "\n",
        "                # check if the model indentified any organizations from the matched text\n",
        "                if len(organizations) != 0:\n",
        "                    # put the pdf file name, identified organizations and matched text into the data list\n",
        "                    for organization in organizations:\n",
        "                        data_organization.append([filename, organization, match_1_text.replace('\\n',' ')])\n",
        "        \n",
        "                else: \n",
        "                    # if the model failed to identify any organizations from the matched text\n",
        "                    # get the text of the first page, which is the cover page of a proposal, and in most cases it contains the client name\n",
        "                    extracted_text = extract_text_from_pages(pdf_dir + filename, 1)\n",
        "                    \n",
        "                    # search for the pattern_2 in the first page\n",
        "                    match_2 = re.search(pattern_2, extracted_text)\n",
        "                    \n",
        "                    if match_2:\n",
        "                        # get the text before \"Proposal\"\n",
        "                        organization = match_2.group(0).strip()\n",
        "                        data_organization.append([filename, organization, extracted_text.replace('\\n',' ')])\n",
        "        \n",
        "                    else: \n",
        "                        # put 'Organization Not Found' message into the data list if failed to match pattern_2\n",
        "                        data_organization.append([filename, 'Organization Not Found', extracted_text.replace('\\n',' ')])\n",
        "                        \n",
        "            else:\n",
        "                # get the text of the first page if we didn't find the matched pattern_1 from the first 2 pages\n",
        "                extracted_text = extract_text_from_pages(pdf_dir + filename, 1)\n",
        "            \n",
        "                # search for the pattern_2 in the first page\n",
        "                match_2 = re.search(pattern_2, extracted_text)\n",
        "                \n",
        "                if match_2:\n",
        "                    # get the text before \"Proposal\"\n",
        "                    organization = match_2.group(0).strip()\n",
        "                    data_organization.append([filename, organization, extracted_text.replace('\\n',' ')])\n",
        "        \n",
        "                else: \n",
        "                    # put 'Organization Not Found' message into the data list if failed to match pattern_2\n",
        "                    data_organization.append([filename, 'Organization Not Found', extracted_text.replace('\\n',' ')])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pG42IvvolEuM"
      },
      "outputs": [],
      "source": [
        "# convert the data list to a pandas DataFrame\n",
        "df_organization = pd.DataFrame(data_organization, columns=['Proposal File Name', 'Client Name', 'Snippet']) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AuxRraQKlEuM"
      },
      "source": [
        "## 2. Extract Proposal Dates From Proposals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MK_RsuDalEuM"
      },
      "outputs": [],
      "source": [
        "# use regular expression to describe the proposal date pattern\n",
        "date_pattern = r'(?:January|February|March|April|May|June|July|August|September|October|November|December)\\s\\d{1,2}[, ]*\\d{4}|\\b\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}\\b|\\b\\d{4}[/-]\\d{1,2}[/-]\\d{1,2}\\b'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kbBC36u0lEuM"
      },
      "outputs": [],
      "source": [
        "# create an empty list to store the data that need to be exported to excel\n",
        "data_date = []\n",
        "\n",
        "# loop through all pdf files in a folder\n",
        "for filename in os.listdir(pdf_dir):\n",
        "    if filename.endswith('.pdf'):\n",
        "        \n",
        "        # extract the first page\n",
        "        extracted_text = extract_text_from_pages(pdf_dir + filename, 1)\n",
        "        \n",
        "        # check if we can extract text from the pdf file\n",
        "        if extracted_text == 'PDF Access Denied':\n",
        "            # put the message into the data list\n",
        "            data_date.append([filename, extracted_text, extracted_text])\n",
        "        \n",
        "        else:\n",
        "            # clean the text\n",
        "            extracted_text = re.sub(r'\\s+', ' ', extracted_text)\n",
        "\n",
        "            # search for date pattern and put identified dates into the data list\n",
        "            dates = re.findall(date_pattern, extracted_text)\n",
        "            for date in dates:\n",
        "                data_date.append([filename, date, extracted_text])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sv-AX6eblEuN"
      },
      "outputs": [],
      "source": [
        "# convert the data list to a pandas DataFrame\n",
        "df_date = pd.DataFrame(data_date, columns=['Proposal File Name', 'Proposal Date', 'Snippet']) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQ3k3MzRlEuN"
      },
      "source": [
        "## 3. Export Data into Excel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "asmkJfFtlEuN"
      },
      "outputs": [],
      "source": [
        "# export dataframes to an excel file\n",
        "with pd.ExcelWriter('spaCy_NER.xlsx', engine='openpyxl') as writer:\n",
        "\n",
        "    # create the first sheet\n",
        "    writer.book.create_sheet('Client Name')\n",
        "    df_organization.to_excel(writer, sheet_name = 'Client Name', index=True)\n",
        "\n",
        "    # create the second sheet\n",
        "    writer.book.create_sheet('Proposal Date')\n",
        "    df_date.to_excel(writer, sheet_name = 'Proposal Date', index=True)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "156f58429f0d5810d9976652445d0c94db6bbf4a608a82c8f9b08792ff11bffc"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}